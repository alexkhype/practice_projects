# ‚ö° REFERENCIA R√ÅPIDA: COMANDOS PYTHON M√ÅS UTILIZADOS EN ENTREVISTAS

## Versi√≥n 1.0 | Cheatsheet para Entrevista

---

## üìö LIBRER√çAS ESENCIALES

```python
import pandas as pd                    # Manipulaci√≥n de datos
import numpy as np                     # Operaciones num√©ricas
import matplotlib.pyplot as plt        # Visualizaci√≥n b√°sica
import seaborn as sns                  # Visualizaci√≥n avanzada

# Machine Learning
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                             roc_auc_score, mean_squared_error, mean_absolute_error, r2_score)
```

---

## üî¥ CARGA DE DATOS

```python
# CSV
df = pd.read_csv('archivo.csv')

# Excel
df = pd.read_excel('archivo.xlsx', sheet_name='Sheet1')

# JSON
df = pd.read_json('archivo.json')

# SQL
from sqlalchemy import create_engine
engine = create_engine('mysql+pymysql://user:pass@localhost/db')
df = pd.read_sql('SELECT * FROM tabla', con=engine)

# Con par√°metros √∫tiles
df = pd.read_csv('datos.csv', sep=',', encoding='utf-8', nrows=1000)
```

---

## üîç EXPLORACI√ìN R√ÅPIDA

```python
df.head()               # Primeras 5 filas
df.tail()               # √öltimas 5 filas
df.info()               # Tipos y no-nulos
df.describe()           # Estad√≠sticas
df.shape                # Dimensiones (filas, columnas)
df.dtypes               # Tipos por columna
df.columns              # Nombres de columnas
df.isnull().sum()       # Conteo de NaN
df.duplicated().sum()   # Conteo de duplicados
df['col'].value_counts() # Frecuencias
```

---

## üßπ LIMPIEZA DE DATOS

### Valores Faltantes
```python
df.isnull().sum()                     # Detectar
df.dropna()                           # Eliminar filas con NaN
df.fillna(df.mean())                  # Llenar con media
df['col'].fillna(df['col'].median())  # Mediana de columna
df['col'].fillna('Desconocido')       # Valor espec√≠fico
df.fillna(method='ffill')             # Forward fill (series temporales)
```

### Duplicados
```python
df.drop_duplicates()                  # Eliminar filas duplicadas
df.drop_duplicates(subset=['col1'])   # Considerar columnas espec√≠ficas
df.duplicated().sum()                 # Contar duplicados
```

### Outliers (IQR)
```python
Q1 = df['col'].quantile(0.25)
Q3 = df['col'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5*IQR
upper = Q3 + 1.5*IQR
df_clean = df[(df['col'] >= lower) & (df['col'] <= upper)]
```

### Tipos de Datos
```python
df['col'] = pd.to_numeric(df['col'], errors='coerce')
df['col'] = df['col'].astype('int64')
df['fecha'] = pd.to_datetime(df['fecha'])
df['col'] = df['col'].astype('category')
```

### Strings
```python
df['col'] = df['col'].str.strip()              # Eliminar espacios
df['col'] = df['col'].str.lower()              # Min√∫sculas
df['col'] = df['col'].str.replace('old', 'new')
df['col'] = df['col'].str.contains('pattern')  # Buscar patr√≥n
```

---

## üî¢ CODIFICACI√ìN DE VARIABLES

### Label Encoding (Ordinales)
```python
# Manual (cuando hay orden: Low < Medium < High)
mapping = {'Low': 0, 'Medium': 1, 'High': 2}
df['col_encoded'] = df['col'].map(mapping)

# Autom√°tico (orden alfab√©tico)
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['col_encoded'] = le.fit_transform(df['col'])
```

### One-Hot Encoding (Nominales)
```python
# Pandas
df_encoded = pd.get_dummies(df, columns=['col'], drop_first=True)

# Sklearn
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(sparse_output=False)
X_encoded = ohe.fit_transform(df[['col']])
```

---

## ‚öñÔ∏è ESCALADO Y NORMALIZACI√ìN

### StandardScaler (Z-score)
```python
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Solo transform en test
```

### MinMaxScaler (0-1)
```python
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### RobustScaler (ante outliers)
```python
from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()
X_scaled = scaler.fit_transform(X_train)
```

---

## ‚úÇÔ∏è DIVISI√ìN TRAIN/TEST

```python
# B√°sica
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Con stratify (clasificaci√≥n desbalanceada)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# K-Fold Cross-Validation
from sklearn.model_selection import KFold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
for train_idx, test_idx in kf.split(X):
    X_train_fold, X_test_fold = X[train_idx], X[test_idx]
    y_train_fold, y_test_fold = y[train_idx], y[test_idx]
    # Entrenar y evaluar aqu√≠
```

---

## ü§ñ MODELOS COMUNES

### Clasificaci√≥n

**Logistic Regression (Baseline)**
```python
from sklearn.linear_model import LogisticRegression
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

**Random Forest**
```python
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(
    n_estimators=100, max_depth=10, random_state=42, n_jobs=-1
)
model.fit(X_train, y_train)
```

**Gradient Boosting**
```python
from sklearn.ensemble import GradientBoostingClassifier
model = GradientBoostingClassifier(
    n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42
)
model.fit(X_train, y_train)
```

**SVM**
```python
from sklearn.svm import SVC
model = SVC(kernel='rbf', C=1.0, random_state=42)
model.fit(X_train, y_train)
```

### Regresi√≥n

**Linear Regression**
```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

**Random Forest Regressor**
```python
from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42)
model.fit(X_train, y_train)
```

---

## üìä EVALUACI√ìN CLASIFICACI√ìN

```python
# Predicciones
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

# Accuracy
from sklearn.metrics import accuracy_score
acc = accuracy_score(y_test, y_pred)

# Precisi√≥n
from sklearn.metrics import precision_score
prec = precision_score(y_test, y_pred)

# Recall
from sklearn.metrics import recall_score
rec = recall_score(y_test, y_pred)

# F1-Score
from sklearn.metrics import f1_score
f1 = f1_score(y_test, y_pred)

# ROC-AUC
from sklearn.metrics import roc_auc_score
auc = roc_auc_score(y_test, y_pred_proba)

# Reporte completo
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

# Matriz de confusi√≥n
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
```

---

## üìà EVALUACI√ìN REGRESI√ìN

```python
# Predicciones
y_pred = model.predict(X_test)

# MAE (Mean Absolute Error)
from sklearn.metrics import mean_absolute_error
mae = mean_absolute_error(y_test, y_pred)

# MSE (Mean Squared Error)
from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)

# RMSE (Root Mean Squared Error)
rmse = np.sqrt(mse)

# R¬≤ Score
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)

# Combo print
print(f"MAE:  {mae:.4f}")
print(f"MSE:  {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"R¬≤:   {r2:.4f}")
```

---

## üîß VALIDACI√ìN CRUZADA

```python
# cross_val_score
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
print(f"CV Scores: {scores.mean():.3f} (+/- {scores.std():.3f})")

# Scoring options
# Clasificaci√≥n: 'accuracy', 'precision', 'recall', 'f1', 'roc_auc'
# Regresi√≥n: 'r2', 'neg_mean_squared_error', 'neg_mean_absolute_error'
```

---

## üéØ OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS

### Grid Search
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15],
    'min_samples_split': [2, 5, 10]
}

grid = GridSearchCV(model, param_grid, cv=5, scoring='f1', n_jobs=-1)
grid.fit(X_train, y_train)

print(f"Best params: {grid.best_params_}")
print(f"Best score: {grid.best_score_:.4f}")
best_model = grid.best_estimator_
```

### Random Search
```python
from sklearn.model_selection import RandomizedSearchCV

random = RandomizedSearchCV(
    model, param_grid, n_iter=20, cv=5, scoring='f1', n_jobs=-1, random_state=42
)
random.fit(X_train, y_train)
```

---

## üå≥ FEATURE IMPORTANCE

```python
# Despu√©s de entrenar Random Forest o Gradient Boosting
feature_importance = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

# Visualizar
import matplotlib.pyplot as plt
plt.barh(feature_importance['feature'], feature_importance['importance'])
plt.xlabel('Importancia')
plt.show()

# Top 5
print(feature_importance.head())
```

---

## üíæ GUARDAR Y CARGAR MODELOS

```python
import joblib

# Guardar
joblib.dump(model, 'modelo.pkl')

# Cargar
model_cargado = joblib.load('modelo.pkl')

# Predicciones con modelo cargado
y_pred = model_cargado.predict(X_new)
```

---

## üöÄ PIPELINE (RECOMENDADO)

```python
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('model', RandomForestClassifier(random_state=42))
])

# Entrenar
pipeline.fit(X_train, y_train)

# Predecir (scaler se aplica autom√°ticamente)
y_pred = pipeline.predict(X_test)

# Con GridSearch
param_grid = {'model__n_estimators': [50, 100], 'model__max_depth': [5, 10]}
grid = GridSearchCV(pipeline, param_grid, cv=5)
grid.fit(X_train, y_train)
```

---

## üìã FEATURE ENGINEERING COM√öN

```python
# Variables polin√≥micas
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X_train)

# Variables de interacci√≥n
df['X1_x_X2'] = df['X1'] * df['X2']

# Variables temporales
df['a√±o'] = df['fecha'].dt.year
df['mes'] = df['fecha'].dt.month
df['dia_semana'] = df['fecha'].dt.dayofweek

# Variables logar√≠tmicas
df['log_precio'] = np.log(df['precio'] + 1)

# Variables binarias
df['es_adulto'] = (df['edad'] >= 18).astype(int)

# Variables de agregaci√≥n
grupo_stats = df.groupby('categoria')['precio'].agg(['mean', 'std']).reset_index()
df = df.merge(grupo_stats, on='categoria', how='left')
```

---

## üé® VISUALIZACI√ìN R√ÅPIDA

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Histograma
plt.hist(df['col'])

# Scatter
plt.scatter(df['col1'], df['col2'])

# Box plot
sns.boxplot(data=df, x='categoria', y='valor')

# Heatmap correlaci√≥n
sns.heatmap(df.corr(), annot=True)

# Distribuci√≥n
sns.histplot(df['col'], kde=True)

# Mostrar
plt.show()
```

---

## ‚úÖ CHECKLIST ANTES DE ENTREGAR

```python
# ‚úì Datos limpios (sin NaN, duplicados, outliers tratados)
# ‚úì Features codificados correctamente
# ‚úì Escalado aplicado
# ‚úì Train/test split apropiado
# ‚úì Cross-validation verificada
# ‚úì M√©trica correcta para el problema
# ‚úì Random state fijado (reproducibilidad)
# ‚úì Test performance verificado (no overfitting)
# ‚úì Feature importance analizado
# ‚úì Modelo guardado y puede ser cargado
```

---

## üí° TIPS FINALES

- Siempre verificar proporciones de clases con `value_counts()`
- Nunca escalar antes de dividir train/test
- Usar `stratify=y` si datos desbalanceados
- Comparar train vs test performance
- Documentar por qu√© cada decisi√≥n
- Ser consistente con `random_state`
- Probar m√∫ltiples modelos, no solo uno

---

**√öltima actualizaci√≥n**: Diciembre 2025
**Versi√≥n**: 1.0