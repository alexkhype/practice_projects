{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7cabf0f-1250-4844-8bce-2e339666329b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "volunteer = pd.read_csv('volunteer_opportunities.csv')\n",
    "wine = pd.read_csv('wine_types.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7e506-0d74-437d-92a8-8285b7afaff1",
   "metadata": {},
   "source": [
    "Data preprocessing comes after we've explored and cleaned our dataset, so we understand its contents, structure, and quality. \n",
    "\n",
    " preprocessing as a prerequisite for modeling.\n",
    "\n",
    " Good practices:\n",
    "* Llamar al df\n",
    "* Ver .head()\n",
    "* Ver .info()\n",
    "* Ver .describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95058e75-1f54-46cf-a7ab-8e89875c8314",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e99e6-ecbd-466d-aa8f-fc0b3b87d185",
   "metadata": {},
   "source": [
    "## 1. Manejo de datos faltantes\n",
    "\n",
    "1. Cuántos datos faltan\n",
    "\n",
    "    * `df.isna().sum()`\n",
    "\n",
    "2. Borrar en caso necesario\n",
    "\n",
    "    * `df.dropna()` general, good option if only a small number or rows contain missing data\n",
    "    \n",
    "    * `df.drop([1, 2, 3])` rows\n",
    "\n",
    "    * `df.drop('A', axis=1))` columnas\n",
    "\n",
    "    * `df.dropna(subset=['B'])` de un subset\n",
    "\n",
    "    * `df.dropna(thresh=2)` especifica cuántos se pueden quedar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e212a1-d125-44b7-97db-8ffa92576c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(617, 33)\n"
     ]
    }
   ],
   "source": [
    "# Drop the Latitude and Longitude columns from volunteer\n",
    "volunteer_cols = volunteer.drop(['Latitude','Longitude'], axis=1)\n",
    "\n",
    "# Drop rows with missing category_desc values from volunteer_cols\n",
    "volunteer_subset = volunteer_cols.dropna(subset=['category_desc'])\n",
    "\n",
    "# Print out the shape of the subset\n",
    "print(volunteer_subset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cafcfe-44b2-4e84-b1d7-da65c86912ef",
   "metadata": {},
   "source": [
    "## 2. Transformar los datos en los tipos adecuados\n",
    "\n",
    "* df.dtypes.value_counts() para saber cuántos hay de cada tipo\n",
    "  \n",
    "* df.info() info general\n",
    "  \n",
    "* df.dtypes  solo tipo de columna\n",
    "  \n",
    "* df['C'] = df['C'].astype('float') # convertir en el tipo adecuadonota que el tipo está entre comillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5fe9e35-c787-4993-9447-dd94c6e1854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    737\n",
      "1     22\n",
      "2     62\n",
      "3     14\n",
      "4     31\n",
      "Name: hits, dtype: int64\n",
      "object     14\n",
      "float64    13\n",
      "int64       8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the head of the hits column\n",
    "print(volunteer[\"hits\"].head())\n",
    "\n",
    "# Convert the hits column to type int\n",
    "volunteer[\"hits\"] = volunteer['hits'].astype('int')\n",
    "\n",
    "# Look at the dtypes of the dataset\n",
    "print(volunteer.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d7727-1ca5-4f18-8b9a-426a1ca7f391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88414d-141a-4866-8b3e-01444eb4054d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2077a487-ffa7-4e3f-a546-4fa69c878bee",
   "metadata": {},
   "source": [
    "## 3. Dividir df\n",
    "\n",
    "    * train_test_split() (con stratify si es necesario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d000560-d3b2-43f3-a14a-a12eeef1cda1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m y \u001b[38;5;241m=\u001b[39m volunteer[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_desc\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Use stratified sampling to split up the dataset according to the y dataset\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Print the category_desc counts from y_train\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_desc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalue_counts())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2670\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2666\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2668\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2670\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2673\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2674\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2675\u001b[0m     )\n\u001b[1;32m   2676\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2229\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m \n\u001b[1;32m   2198\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2227\u001b[0m \u001b[38;5;124;03m    to an integer.\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2229\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:107\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 107\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput contains NaN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(X\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex floating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create a DataFrame with all columns except category_desc\n",
    "X = volunteer.dropna()\n",
    "X = volunteer.drop('category_desc', axis=1)\n",
    "\n",
    "# Create a category_desc labels dataset\n",
    "y = volunteer[['category_desc']]\n",
    "\n",
    "# Use stratified sampling to split up the dataset according to the y dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "# Print the category_desc counts from y_train\n",
    "print(y_train['category_desc'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d968139d-2f14-4a84-822d-7cd5a78c37cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Fit the knn model to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Score the model on the test data\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cf86df-1506-4ee6-a6c2-7a72140a6057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23217994-c97f-4be8-b47f-359468f81463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "906b4b62-16d7-49ef-9154-438a4bf7f77a",
   "metadata": {},
   "source": [
    "4. Standardization: \"normaliza\" distribuciones que no son estandares, especialmente para los modelos de scikit-learn. esto es para datos numéricos continuos, NO CATEGORICOS. The better practice is to perform standardization (or any feature scaling/normalization) after splitting the dataset into training and test sets. This approach prevents data leakage because the test data is kept completely separate and unseen during the fitting of the scaler\n",
    "\n",
    "    * ¿Cuándo se usa?:\n",
    "  \n",
    "        * if we're working with any kind of model that uses a linear distance metric or operates in a linear space like k-nearest neighbors, linear regression, or k-means clustering, the model is assuming that the data and features we're giving it are related in a linear fashion, or can be measured with a linear distance metric, which may not always be the case.\n",
    "        * Standardization should also be used when dataset features have a high variance, which is also related to distance metrics. **If a feature in our dataset has a variance that's an order of magnitude or more greater than the other features,** (¿qué significa eso?) this could impact the model's ability to learn from other features in the dataset.\n",
    "        * Modeling a dataset that contains continuous features that are on different scales is another standardization scenario. For example, consider predicting house prices using two features: the number of bedrooms and the last sale price.\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49fb4a7-efdd-47df-9fc6-c694948d66df",
   "metadata": {},
   "source": [
    " * log normalization: useful when we have features with high variance.  Log normalization is a good strategy when you care about relative changes in a linear model, but still want to capture the magnitude of change, and when we want to keep everything in the positive space. It's a nice way to minimize the variance of a column and make it comparable to other columns for modeling.\n",
    "  \n",
    "        * observar la varianza de las columnas:\n",
    "          `df.var()`\n",
    "        * aplicar la función de numpy:\n",
    "          `df['log_2'] = np.log(df['col2'])` es buena práctica crear otra columna que indique que tiene los datos normalizados\n",
    "      \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40072caf-81b9-4380-b2c9-73064fde03cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99166.71735542436\n",
      "0.17231366191842012\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Print out the variance of the Proline column\n",
    "print(wine['Proline'].var())\n",
    "\n",
    "# Apply the log normalization function to the Proline column\n",
    "wine['Proline_log'] = np.log(wine['Proline'])\n",
    "\n",
    "# Check the variance of the normalized Proline column\n",
    "print(wine['Proline_log'].var())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fccfff1-2f46-43eb-973b-b8393a0ed9d2",
   "metadata": {},
   "source": [
    " * scaling: Scaling is a method of standardization that's most useful when we're working with a dataset that contains continuous features that are on different scales, and we're using a model that operates in some sort of linear space (like linear regression or k-nearest neighbors). Feature scaling transforms the features in your dataset so they have a mean of zero and a variance of one. we have numbers that have consistent scales within columns, but not across columns. If we look at the variance, it's relatively low across columns.\n",
    "  \n",
    "    * escalado:\n",
    "  \n",
    "        from sklearn.preprocessing import StandardScaler # importar\n",
    "        scaler = StandardScaler() # crear una variable con el escalador\n",
    "        df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns) # crear nueva columna, usar fit_transform para mayor practicidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a6bd989-9ab7-4939-a334-cd6c4ea8ac52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ash                    0.075265\n",
      "Alcalinity of ash     11.152686\n",
      "Magnesium            203.989335\n",
      "dtype: float64\n",
      "0    1.00565\n",
      "1    1.00565\n",
      "2    1.00565\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Subset the DataFrame you want to scale \n",
    "wine_subset = wine[['Ash', 'Alcalinity of ash', 'Magnesium']]\n",
    "\n",
    "# Apply the scaler to wine_subset\n",
    "wine_subset_scaled = scaler.fit_transform(wine_subset)\n",
    "\n",
    "print(wine_subset.var())\n",
    "print(pd.DataFrame(wine_subset_scaled).var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9210e0a4-52f2-418e-97b4-492d61ab2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EJEMPLO\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) \n",
    "\n",
    "'''\n",
    "# Using the transform method means that the test features \n",
    "won't be used to fit the model and avoids data leakage. \n",
    "'''\n",
    "\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "knn.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b674daf-d099-4852-b6a8-432cce05b136",
   "metadata": {},
   "source": [
    "TRATAR DE APLICAR ESO AL DE VINOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f29478-e036-4f51-9c67-a1ec82afd18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
