{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1aff4e5-6a6c-4bfe-a65d-3282c70bc622",
   "metadata": {},
   "source": [
    "# Prueba de cordura"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738905e1-8e34-4a62-a345-692635d24e10",
   "metadata": {},
   "source": [
    "Las pruebas de cordura (sanity tests) son un subconjunto de pruebas de regresión que se realizan para verificar rápidamente que ciertas funcionalidades específicas de un sistema funcionan después de un cambio o actualización.\n",
    "\n",
    "Para hacer la prueba de cordura al modelo para buscar problemas de clasificación necesitamos comparar sus predicciones con posibilidad aleatoria. ¿Cómo se hace una prueba de cordura para un modelo de regresión?\n",
    "\n",
    "Darle la misma respuesta a todas las observaciones es un método simple de predicción de regresión. Vamos a usar el valor promedio del precio del apartamento para estar más cerca de la realidad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d96c61-c000-45c2-ae9f-196e34116e79",
   "metadata": {},
   "source": [
    "1. encuentra el precio promedio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88c4abf3-6433-4b7d-bd65-7431dd055206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average price: 161005.67427559663\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('https://practicum-content.s3.us-west-1.amazonaws.com/datasets/train_data_us.csv')\n",
    "\n",
    "# < crea las variables features y target >\n",
    "features = df.drop(['last_price'], axis=1)\n",
    "target = df['last_price']\n",
    "\n",
    "# < calcula e imprime el promedio >\n",
    "print('Average price:', target.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc032f-a147-4f27-9f7f-8b400c2a906e",
   "metadata": {},
   "source": [
    "2. Calcula el MSE o RMSE para el conjunto de entrenamiento usando el precio promedio como valor de predicción. Estos son los valores que los modelos deben superar para validar su buen funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d282906-315d-4b32-8949-c05c3c137a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 5.529775874409322\n",
      "RMSE: 2.3515475488302\n"
     ]
    }
   ],
   "source": [
    "# Aquí está haciendo que todas las predictions sean iguales\n",
    "predictions = pd.Series(target.mean(), index=target.index)\n",
    "\n",
    "# < calcula el ECM  >\n",
    "mse = mean_squared_error(target, predictions)\n",
    "rmse = mean_squared_error(target, predictions, squared=False)\n",
    "\n",
    "print('MSE:', mse)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2477e58e-26c7-4a97-bb6f-582e345d917a",
   "metadata": {},
   "source": [
    "# Métricas para modelos de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93762f44-cdbd-43a0-8ea4-aebbcee5c12e",
   "metadata": {},
   "source": [
    "| Métrica               | Explicación                                                                 | ¿Por qué elegirla?                                         | Librería Python      | Función de llamada                   |\n",
    "|-----------------------|-----------------------------------------------------------------------------|------------------------------------------------------------|---------------------|------------------------------------|\n",
    "| Mean Squared Error (MSE) | Promedio de los errores al cuadrado entre valores reales y predichos.        | Penaliza errores grandes, muy usada para regresión.         | sklearn.metrics     | mean_squared_error(y_true, y_pred) |\n",
    "| Root Mean Squared Error (RMSE) | Raíz cuadrada del MSE.                                                     | En la misma escala que las variables, interpretación fácil. | sklearn.metrics | mean_squared_error(y_true, y_pred, squared=False)     |\n",
    "| Mean Absolute Error (MAE) | Promedio del valor absoluto de los errores entre reales y predichos.         | Más robusta a valores atípicos que MSE.                     | sklearn.metrics     | mean_absolute_error(y_true, y_pred)|\n",
    "| R2 (Coeficiente de determinación) | Mide proporción de la varianza explicada por el modelo.                | Indica ajuste global del modelo, interpretabilidad alta.    | sklearn.metrics     | r2_score(y_true, y_pred)            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bf782a7-8622-40b9-9d1d-2ba1f3a8be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = [623, 253, 150, 237]\n",
    "predictions = [649, 253, 370, 148]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d8127-b9e4-4709-81c4-5876d4e7ddd8",
   "metadata": {},
   "source": [
    "# Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34e7e099-6c20-45a4-b555-f0a94e0664d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14249.25\n"
     ]
    }
   ],
   "source": [
    "# Función manual de MSE\n",
    "\n",
    "def mse(answers, predictions):\n",
    "    total = 0\n",
    "    for i in range(len(answers)):\n",
    "        total += (answers[i] - predictions[i])**2 # < agrega el error cuadrático >\n",
    "    result = total/len(answers) # < divide la suma entre el número de observaciones >\n",
    "    return result \n",
    "\n",
    "print(mse(answers, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "186841e1-a694-4573-a5d2-9f50c7f17eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14249.25\n"
     ]
    }
   ],
   "source": [
    "# De la librería sklearn.metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "result = mean_squared_error(answers, predictions)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4c08d4-a739-485e-9034-9fdfc746f07c",
   "metadata": {},
   "source": [
    "# Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83eba9d8-e4a1-4199-9746-f2f2d43516ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.37022241748568\n"
     ]
    }
   ],
   "source": [
    "# Función manual de MSE\n",
    "\n",
    "def mse(answers, predictions):\n",
    "    total = 0\n",
    "    for i in range(len(answers)):\n",
    "        total += (answers[i] - predictions[i])**2 # < agrega el error cuadrático >\n",
    "    result = (total/len(answers)) ** 0.5 # < divide la suma entre el número de observaciones >\n",
    "    return result \n",
    "\n",
    "print(mse(answers, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bbb8bdb-9c3a-4caf-8a4b-b23e10c45c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119.37022241748568\n"
     ]
    }
   ],
   "source": [
    "# De la librería sklearn.metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "result = mean_squared_error(answers, predictions, squared=False)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db79933-40a0-4366-8409-10febacdf95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8defc898-6ee4-4a62-bc0f-8c50d80b0682",
   "metadata": {},
   "source": [
    "# Métricas para modelos de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305b797-4641-4dc9-91e1-521e7e7349ee",
   "metadata": {},
   "source": [
    "| Métrica               | Explicación                                                                | ¿Por qué elegirla?                                           | Librería Python      | Función de llamada                   |\n",
    "|-----------------------|-----------------------------------------------------------------------------|--------------------------------------------------------------|---------------------|------------------------------------|\n",
    "| Accuracy (Exactitud)   | Porcentaje de predicciones correctas totales.                             | Intuitiva, buena para datos balanceados.                     | sklearn.metrics     | accuracy_score(y_true, y_pred)      |\n",
    "| Precision (Precisión)  | Proporción de verdaderos positivos entre todos los positivos predichos.   | Útil cuando falsos positivos son costosos o importantes.      | sklearn.metrics     | precision_score(y_true, y_pred)     |\n",
    "| Recall (Sensibilidad) | Proporción de verdaderos positivos detectados entre todos los positivos reales. | Útil cuando es crítico detectar la mayoría de positivos.       | sklearn.metrics     | recall_score(y_true, y_pred)        |\n",
    "| F1 Score              | Media armónica entre precisión y recall.                                  | Para balancear precisión y recall en datos desbalanceados.   | sklearn.metrics     | f1_score(y_true, y_pred)             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56256753-7d6e-42c9-82fe-35955e4eb897",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3062ec-758f-480c-9d13-5ee70fd0cbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errores: 507\n",
      "Accuracy: 0.8959145965920755\n"
     ]
    }
   ],
   "source": [
    "# VERSION 1: Con funciones\n",
    "\n",
    "target_predictions = model_DTC.predict(features_train)\n",
    "\n",
    "def error_count(answers, predictions):\n",
    "    count = 0\n",
    "    for i in range(len(answers)):\n",
    "        if answers[i] != predictions[i]:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def accuracy(answers, predictions):\n",
    "    errors = error_count(answers, predictions)\n",
    "    return (len(answers) - errors) / len(predictions)\n",
    "\n",
    "print('Errores:', error_count(target_train.values, target_predictions))\n",
    "print('Accuracy:', accuracy(target_train.values, target_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e4b265-6a68-433e-8120-2a3cea376f69",
   "metadata": {},
   "source": [
    "AQUI\n",
    "\n",
    "PRECISION (PRECISION)\n",
    "SENSIBILIDAD (RECALL)\n",
    "EXACTITUD (ACCURACY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5aa3df9d-e492-408f-8759-31d2146bf59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8959145965920755\n"
     ]
    }
   ],
   "source": [
    "# VERSION 2: Con una librería\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "target_predictions = model_DTC.predict(features_train)\n",
    "\n",
    "accuracy = accuracy_score(target_train, target_predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd2c42-099c-4b64-9716-92d35f7efb00",
   "metadata": {},
   "source": [
    "accuracy_score is a standalone function from sklearn.metrics that directly compares the true labels (target_train) with predicted labels (target_predictions). It requires you to first generate predictions externally (e.g., using model.predict(features_train)) and then calculate the accuracy. It is more flexible because you can compute accuracy for any set of true and predicted labels, including predictions from different models or manual predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d65ab10-3bf0-4e3f-b53f-27fd5946c2d1",
   "metadata": {},
   "source": [
    "Explorar más esa librería"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d804c6-c761-4d7c-9867-bf8271ed8c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8959145965920755\n"
     ]
    }
   ],
   "source": [
    "# OPCIÓN 3\n",
    "\n",
    "score = model_DTC.score(features_train, target_train) \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf655c-1d93-4d79-a983-1b0da24e3a78",
   "metadata": {},
   "source": [
    "model.score is a method of the trained model object that internally makes predictions on the input features (features_train) and compares those predictions to the true labels (target_train) to compute the accuracy. It combines prediction and accuracy calculation in one call and is less flexible but more convenient for quick evaluation directly on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f29e4-5a47-430a-ace3-eea520415b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ce3b20-631e-4e1c-ace1-ff09860f47dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9b571a-5d8d-4383-941c-9655d5af6d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b042f-06fc-45b4-b7e6-94b31f897426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac6729-bedf-4b81-801c-e4bf8ae93fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
